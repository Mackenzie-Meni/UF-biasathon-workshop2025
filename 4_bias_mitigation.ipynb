{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tableone import TableOne\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading original datasets\n",
    "original_data = pd.read_csv('../preprocessed_data.csv')\n",
    "\n",
    "# Loading altered datasets\n",
    "data_altered_20_percent_african_american= pd.read_csv('../data_altered_20_percent_african_american.csv')\n",
    "data_altered_80_percent_female = pd.read_csv('../data_altered_80_percent_female.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the dataset you want to experiment with and split the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (62060, 38)\n",
      "X_test shape: (15515, 38)\n",
      "y_train shape: (62060,)\n",
      "y_test shape: (15515,)\n"
     ]
    }
   ],
   "source": [
    "# choose the data set to experiment with\n",
    "data = original_data\n",
    "\n",
    "# Drop the 'gender_M' and 'ethnicity_Other' columns\n",
    "data = data.drop(columns=['gender_M', 'ethnicity_Other/Unknown'])\n",
    "\n",
    "# Define your features and target variable\n",
    "X = data.drop('hospital_death', axis=1)  # Features\n",
    "y = data['hospital_death']               # Target variable\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Original\n",
    "original_rf_loaded = joblib.load(\"../original_data_random_forest_model.pkl\")\n",
    "\n",
    "# Random Forest Altered 20% African American\n",
    "altered_rf_loaded = joblib.load(\"../altered_rf_model.pkl\")\n",
    "\n",
    "# Random Forest Altered 80% Female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model with upsampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split for Upsampling\n",
    "\n",
    "- With stratify=y, both train and test sets will preserve the ratio of the classes from the original data, making the results more representative and the model's performance more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = original_data\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data = data.drop(columns=['gender_M', 'ethnicity_Other/Unknown'])\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['hospital_death'])\n",
    "y = data['hospital_death']\n",
    "\n",
    "# Split data first (before upsampling)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsampling the minority class for prediction\n",
    "\n",
    "This function balances an imbalanced dataset by duplicating (upsampling) the minority class so that it has the same number of samples as the majority class.\n",
    "\n",
    "**üîç Example Before & After**\n",
    "\n",
    "| Class         | Original Count | After Upsampling |\n",
    "|--------------|---------------|------------------|\n",
    "| Survived (0) | 10,000        | 10,000          |\n",
    "| Died (1)     | 2,000         | 10,000 (upsampled) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **True Upsampling (Duplicate Minority Class)**\n",
    "def upsample_minority(X, y):\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Separate majority and minority classes\n",
    "    majority = df[df['hospital_death'] == 0]\n",
    "    minority = df[df['hospital_death'] == 1]\n",
    "    \n",
    "    print(majority.shape)\n",
    "    print(minority.shape)\n",
    "    # Upsample the minority class by repeating existing samples\n",
    "    minority_upsampled = minority.sample(n=len(majority), replace=True, random_state=42)\n",
    "\n",
    "    # Combine the upsampled minority class with the majority class\n",
    "    upsampled_df = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "    # Shuffle the dataset to mix samples well\n",
    "    upsampled_df = upsampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return upsampled_df.drop(columns=['hospital_death']), upsampled_df['hospital_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the upsampling and train the Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56842, 39)\n",
      "(5218, 39)\n",
      "Accuracy: 0.92\n",
      "AUROC: 0.83\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96     14210\n",
      "           1       0.52      0.27      0.36      1305\n",
      "\n",
      "    accuracy                           0.92     15515\n",
      "   macro avg       0.73      0.63      0.66     15515\n",
      "weighted avg       0.90      0.92      0.91     15515\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13882   328]\n",
      " [  948   357]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../upsampled_random_forest_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply upsampling\n",
    "X_train_upsampled, y_train_upsampled = upsample_minority(X_train, y_train)\n",
    "\n",
    "# Train a Random Forest model on upsampled data\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auroc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"AUROC: {auroc:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, \"../upsampled_random_forest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe and a comparison of the original model vs upsampled model:\n",
    "\n",
    "\n",
    "**Original**\n",
    "\n",
    "| Actual ‚Üì / Predicted ‚Üí | Predicted 0 | Predicted 1 |\n",
    "|------------------------|-------------|-------------|\n",
    "| **Actual 0** (No Death)  | 14027      | 187         |\n",
    "| **Actual 1** (Death)     | 1032         | 269         |\n",
    "\n",
    "\n",
    "**Upsampled Model**\n",
    "\n",
    "| Actual ‚Üì / Predicted ‚Üí | Predicted 0 | Predicted 1 |\n",
    "|------------------------|-------------|-------------|\n",
    "| **Actual 0** (No Death)  | 13,882      | 328         |\n",
    "| **Actual 1** (Death)     | 948         | 357         |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Summary of Comparison\n",
    "\n",
    "|  | **Original Model** | **Upsampled Model** | **Changes in the Upsampled Model Compared to Original** |\n",
    "|--------------------|----------------|----------------|----------------|\n",
    "| **True Positives (Correct Deaths)** | 269 | **357** | **+88 (Better at catching deaths)** |\n",
    "| **False Negatives (Missed Deaths)** | 1,032 | **948** | **-84 (Fewer missed high-risk patients)** |\n",
    "| **False Positives (Wrongly Predicted Deaths)** | 187 | **328** | **+141 (More cautious in predicting risk)** |\n",
    "| **True Negatives (Correctly Predicted Survivals)** | 14,027 | **13,882** | **-145 (Slight decrease in correctly classified survivals)** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the bias per category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which categories to analyze\n",
    "original_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexible Bias Assessment Function\n",
    "This code checks if the AI model is fair by seeing how well it predicts hospital outcomes for different groups, like gender and ethnicity. It looks at accuracy (how often the model is right) and AUROC (how well it separates high-risk from low-risk patients).\n",
    "\n",
    "If the model performs worse for certain groups, it might be biased, meaning it doesn't work equally well for everyone. This helps us identify unfairness and improve the model to make healthcare predictions more fair and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Flexible Bias Assessment Function**\n",
    "def assess_bias(model, X_test, y_test, feature_name):\n",
    "    \"\"\"Evaluate model performance for different demographic groups.\"\"\"\n",
    "    \n",
    "    # Ensure the feature exists in the dataset\n",
    "    if feature_name not in X_test.columns:\n",
    "        print(f\"Skipping {feature_name}: Not found in dataset\")\n",
    "        return pd.DataFrame(columns=[\"Category\", \"Accuracy\", \"AUROC\"])\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Check if model supports `predict_proba` (some models like SVM do not)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = y_pred  # Use predictions directly if no probabilities are available\n",
    "    \n",
    "    # Analyze performance across each category\n",
    "    categories = X_test[feature_name].unique()\n",
    "    results = []\n",
    "\n",
    "    for category in categories:\n",
    "        mask = X_test[feature_name] == category  # Boolean mask\n",
    "        y_true_group = y_test[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        y_proba_group = y_pred_proba[mask]\n",
    "\n",
    "        if len(y_true_group) == 0:\n",
    "            continue  # Skip if no data for this category\n",
    "\n",
    "        accuracy_group = accuracy_score(y_true_group, y_pred_group)\n",
    "        auroc_group = roc_auc_score(y_true_group, y_proba_group) if len(set(y_true_group)) > 1 else np.nan  # Avoid AUROC error for single class\n",
    "\n",
    "        results.append([category, accuracy_group, auroc_group])\n",
    "\n",
    "    return pd.DataFrame(results, columns=[\"Category\", \"Accuracy\", \"AUROC\"])\n",
    "\n",
    "\n",
    "# **Example: Running Bias Analysis for Any Model**\n",
    "\n",
    "    # Choose which features to evaluate\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    features_to_check = [\"gender_F\", \"ethnicity_Caucasian\", \"ethnicity_African American\", \"ethnicity_Hispanic\", \"ethnicity_Native American\"]\n",
    "\n",
    "    for feature in features_to_check:\n",
    "        print(f\"\\nBias Analysis for {feature}:\")\n",
    "        bias_results = assess_bias(model, X_test, y_test, feature)\n",
    "        print(bias_results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating Random Forest:\n",
      "\n",
      "Bias Analysis for gender_F:\n",
      "   Category  Accuracy     AUROC\n",
      "0      True  0.915752  0.828310\n",
      "1     False  0.919453  0.840209\n",
      "\n",
      "Bias Analysis for ethnicity_Caucasian:\n",
      "   Category  Accuracy     AUROC\n",
      "0     False  0.922971  0.848373\n",
      "1      True  0.916169  0.831086\n",
      "\n",
      "Bias Analysis for ethnicity_African American:\n",
      "   Category  Accuracy     AUROC\n",
      "0     False  0.915774  0.834787\n",
      "1      True  0.933923  0.834359\n",
      "\n",
      "Bias Analysis for ethnicity_Hispanic:\n",
      "   Category  Accuracy     AUROC\n",
      "0     False  0.918746  0.835715\n",
      "1      True  0.895062  0.815713\n",
      "\n",
      "Bias Analysis for ethnicity_Native American:\n",
      "   Category  Accuracy     AUROC\n",
      "0     False  0.917978  0.833912\n",
      "1      True  0.893617  0.910000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate both models\n",
    "\n",
    "model = joblib.load('../upsampled_random_forest_model.pkl')\n",
    "\n",
    "print(\"\\nüîç Evaluating Random Forest:\")\n",
    "evaluate_model(rf_model, X_test, y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLClass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
